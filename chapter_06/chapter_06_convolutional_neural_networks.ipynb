{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34006ee1-51e5-4a48-82de-021a71e1201a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# StatQuest: Convolutional Neural Networks!!!\n",
    "\n",
    "Copyright 2024, Joshua Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0aedd-9400-4ce1-8885-d800f997cb6b",
   "metadata": {},
   "source": [
    "---- \n",
    "\n",
    "In this Studio, we will use **[PyTorch](https://pytorch.org/) + [Lightning](https://www.lightning.ai/)** to create and optimize a simple Convolutional Neural Network (CNN) like the one shown in the picture below. Convolutional neural networks are used for image classification, so when your phone thinks you've taken a picture of a cute kitten, that picture was probably classified with a CNN. In this case, we will create a network that can classify Xs and Os during a game of tic-tac-toe.\n",
    "\n",
    "<img src=\"./images/annotated_cnn.png\" alt=\"a simple convolutional neural network\" style=\"width: 600px;\">\n",
    "\n",
    "In this tutorial, you will...\n",
    "\n",
    "- **[Create Data and then Build a DataLoader From Scratch](#data)**\n",
    "- **[Build a Simple Convolutional Neural Network](#build)**\n",
    "- **[Train the Convolutional Neural Network](#train)**\n",
    "- **[Classify New Images](#predict)**\n",
    "\n",
    "#### NOTE:\n",
    "This tutorial assumes that you already know the basics of coding in **Python** and are familiar with the theory behind **[Convolutional Neural Networks](https://youtu.be/HGwBXDKFk9I)** and  **[Backpropagation](https://youtu.be/IN2XmBhILt4)**. If not, check out the **StatQuests** by clicking on the links for each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3669a0-284c-48f7-9c73-19812e2a8fac",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4feccd-8472-4bb2-bc56-ea41e527d714",
   "metadata": {},
   "source": [
    "# Import the modules that will do all the work\n",
    "\n",
    "The very first thing we need to do is load a bunch of Python modules. Python itself is just a basic programming language. These modules give us extra functionality to create and train a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18741e9-1427-4e24-8154-b6cff9cbe91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# %%capture prevents this cell from printing a ton of STDERR stuff to the screen\n",
    "\n",
    "## NOTE: Uncomment the next line to install stuff if you need to.\n",
    "##       Also, installing can take a few minutes...\n",
    "# !pip install lightning seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshstarmer/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "from torch.optim import Adam # optim contains many optimizers. This time we're using Adam\n",
    "\n",
    "import lightning as L # lightning has tons of cool tools that make neural networks easier\n",
    "from torch.utils.data import TensorDataset, DataLoader # these are needed for the training data\n",
    "\n",
    "import matplotlib.pyplot as plt ## matplotlib allows us to draw the images used for input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e66ff-db47-4057-9f50-7c9576ec58ee",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23672f4-c30e-42de-9d35-5fa6b9bfd0f5",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "# Create Images and then Build a DataLoader From Scratch.\n",
    "\n",
    "Once we have the Python modules imported, we need to create the images to train and test our neural network. Specifically, we'll create an image of the letters **O** and **X** that look like the pictures below.\n",
    "\n",
    "<img src=\"./images/training_data.png\" alt=\"a convolutional neural network\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fcc4d3-9dd9-42d8-87ab-aea0472448fc",
   "metadata": {},
   "source": [
    "We'll start by creating the image of the letter **O** by creating a 6x6 matrix of numbers where 0 represents white and 1 represents black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30fc065c-c267-44ba-b62c-4e96f58b9166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1, 1, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 1],\n",
       " [1, 0, 0, 0, 0, 1],\n",
       " [0, 1, 0, 0, 1, 0],\n",
       " [0, 0, 1, 1, 0, 0]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a 6x6 matrix of numbers where 0 represents white\n",
    "## and 1 represents black.\n",
    "o_image = [[0, 0, 1, 1, 0, 0],\n",
    "           [0, 1, 0, 0, 1, 0],\n",
    "           [1, 0, 0, 0, 0, 1],\n",
    "           [1, 0, 0, 0, 0, 1],\n",
    "           [0, 1, 0, 0, 1, 0],\n",
    "           [0, 0, 1, 1, 0, 0]]\n",
    "o_image # print out the matrix to verify that it is what we expect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31198b-5d70-4f39-9ee5-af673732d1e7",
   "metadata": {},
   "source": [
    "Now, let's create an image of the letter **X** by creating a similar 6x6 matrix, where the 1s are now in an **X** pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b28183-d900-458e-aaef-28dfe9c7cec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 1],\n",
       " [0, 1, 0, 0, 1, 0],\n",
       " [0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 1, 1, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_image = [[1, 0, 0, 0, 0, 1],\n",
    "           [0, 1, 0, 0, 1, 0],\n",
    "           [0, 0, 1, 1, 0, 0],\n",
    "           [0, 0, 1, 1, 0, 0],\n",
    "           [0, 1, 0, 0, 1, 0],\n",
    "           [1, 0, 0, 0, 0, 1]]\n",
    "x_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3661a-e60e-4cc0-a06b-efcafab2bc25",
   "metadata": {},
   "source": [
    "By squinting at the matrices we created for `o_image` and `x_image`, we can sort of see that we made them correctly. However, by drawing the images with matplotlib, we can get a much clearer picture, or rather, we can get much clearer pictures, of what we have done so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d27f684-10dd-4887-962e-b89409f3e3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x143af6f70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## To draw the o_image and x_image, we first call subplots(), which creates \n",
    "## an array, called axarr[], with an entry for each element in a grid\n",
    "## specified by nrows and ncols.\n",
    "fig, axarr = plt.subplots(nrows=1, ncols=2, figsize=(5, 5))\n",
    "\n",
    "## Now we pass o_image and x_image to .imshow() for each element\n",
    "## in the grid created by plt.subplots()\n",
    "axarr[0].imshow(o_image, cmap='gray_r') ## Setting cmap='gray_r' gives us reverse grayscale.\n",
    "axarr[1].imshow(x_image, cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05fb7a5-812b-4838-97b1-00e2ae6f59d3",
   "metadata": {},
   "source": [
    "Now, let's put our training data into a **DataLoader**, which we can use to train the neural network. **DataLoaders** are great for large datasets because they make it easy to access the data in batches, make it easy to shuffle the data each epoch, and they make it easy to use a relatively small fraction of the data if we want to do a quick and dirty training for debugging our code.\n",
    "\n",
    "To put our data training data into a **DataLoader**...\n",
    "\n",
    "- We'll start by converting the images into tensors with `torch.tensor()` and save them as `input_images`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0b7cbe-71f3-4d03-b291-bcd0e0efd138",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the images into tensors...\n",
    "input_images = torch.tensor([o_image, x_image]).type(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fedb584-dcb0-4e1b-ba22-f7ebaa40cfb0",
   "metadata": {},
   "source": [
    "**NOTE:** When we call `torch.tensor()`, we tack on `type(torch.float32)` to ensure the numbers are saved in the correct format for the neural network to process efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8dc01-dfe3-4583-99c4-1f3c07c30dfe",
   "metadata": {},
   "source": [
    "- Then, we'll create tensors for the labels, the ideal output values given each input image. In this example, our convolutional neural network has 2 outputs, one for the letter **O** and one for the letter **X**, so `[1.0, 0.0]` will represent the ideal output for the letter **O**, and `[0.0, 1.0]` will represent the ideal output for the letter **X**. The labels will be saved in `input_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4da9669-2834-4702-a708-fe357d0f8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the labels for the input images\n",
    "input_labels = torch.tensor([[1.0, 0.0], [0.0, 1.0]]).type(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9fef8-7737-4262-acbf-f7307d0015bb",
   "metadata": {},
   "source": [
    "- Then, we'll combine `input_images` with `input_labels` to create a **TensorDataset** and use the **TensorDataset** to create the **DataLoader**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3e4d801-a6e2-42c6-9c68-279a0555a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now combine input_images and input_labels into a TensorDataset...\n",
    "dataset = TensorDataset(input_images, input_labels) \n",
    "## ...and use the TensorDataset to create a DataLoader.\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca312e-fb67-4973-8009-9cf0be837a20",
   "metadata": {},
   "source": [
    "Now, just for fun, we can verify that `dataloader` contains the input images and labels by using it in a `for` loop and printing out the images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ce7d8f-c2a5-4859-92ed-30beb59f9a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num: 0\n",
      "tensor([[[0., 0., 1., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 1., 0., 0.]]])\n",
      "tensor([[1., 0.]])\n",
      "\n",
      "batch_num: 1\n",
      "tensor([[[1., 0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 1., 0., 0.],\n",
      "         [0., 0., 1., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0., 0., 1.]]])\n",
      "tensor([[0., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_num, (images, labels) in enumerate(dataloader): \n",
    "    print(\"batch_num:\", batch_num)\n",
    "    print(images)\n",
    "    print(labels)\n",
    "    print() ## print a blank line to separate each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31955e37-156e-4c5b-b4bd-806d0e6e2d74",
   "metadata": {},
   "source": [
    "# BAM!\n",
    "\n",
    "At long last, we have created the **DataLoader** that we will use to train a convolutional neural network. Now, let's build it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fb953-440f-4930-a018-050070e67040",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b97945-a5bb-47d2-a07a-04b0dce01539",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"build\"></a>\n",
    "# Building a convolutional neural network with PyTorch and Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed8a32-4aeb-4a4e-9c6c-4bfff11ba0ea",
   "metadata": {},
   "source": [
    "Building a convolutional neural network with PyTorch means creating a new class. And to make it easy to train the neural network, this class will inherit from `LightningModule`.\n",
    "\n",
    "Our new class will have the following methods:\n",
    "- `__init__()` to initialize the weights and biases and keep track of a few other housekeeping things.\n",
    "- `forward()` to make a forward pass through the convolutional neural network.\n",
    "- `configure_optimizers()` to configure the optimizer. In this tutorial, we'll use `Adam`.\n",
    "- `training_step()` to pass the training data to `forward()`, calculate the loss, and keep track of the loss values in a log file.\n",
    "\n",
    "Here is a picture of the neural network we want to create:\n",
    "\n",
    "<img src=\"./images/annotated_cnn.png\" alt=\"a simple convolutional neural network\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f26851-96df-4a84-b1ce-3351e61adf25",
   "metadata": {},
   "source": [
    "As we can see in the picture, our convolutional neural network accepts a 6x6 image as the input. Then, it applies a 3x3 filter, or kernel, to the image. That is the convolutional step in a convolutional neural network. Anyway, then we run the output from the filter through a **[ReLU](https://youtu.be/68BZ5f7P94E)** activation function before a max pooling step. Lastly, we run the max pooling values through a simple neural network with 4 inputs and 2 outputs.\n",
    "\n",
    "**NOTE:** This specific convolutional neural network is super simple. Usually, the images have multiple color channels (one for red, one for blue, and one for green), and multiple filters are applied to each channel. The output from these filters can then be run through multiple layers of filters. In contrast, our example is super simple, so we'll call it `SimpleCNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7879b36c-947f-4638-9287-d6d1d50e734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now build a simple CNN...\n",
    "class SimpleCNN(L.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__() ## We call the __init__() for the parent, LightningModule, so that it\n",
    "                           ## can initialize itself as well.\n",
    "        \n",
    "        ## Now we set the seed for the random number generorator.\n",
    "        ## This ensures that when you create a model from this class, that model\n",
    "        ## will start off with the exact same random numbers that I started out with when\n",
    "        ## I created this demo. At least, I hope that is what happens!!! :)\n",
    "        L.seed_everything(seed=42)\n",
    "        \n",
    "        ############################################################################\n",
    "        ##\n",
    "        ## Here is where we initialize the Weights and Biases for the CNN\n",
    "        ##\n",
    "        ############################################################################\n",
    "        \n",
    "        ## The filter is created and applied by nn.Conv2d().\n",
    "        ## in_channels - The number of color channels that\n",
    "        ##    the image has. Our black and white image only \n",
    "        ##    has one channel. However, color pictures usually have 3.\n",
    "        ## out_channels - If we had multiple input channels, we could merge\n",
    "        ##    them down to one output. Or we can increase the number of\n",
    "        ##    output channels if we want.\n",
    "        ## kernel_size - The size of the filter (aka kernel). In this case\n",
    "        ##    we want a 3x3 filter, but you can select all kinds of sizes,\n",
    "        ##    including sizes that are more rectangular than square.\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)\n",
    "        \n",
    "        ## nn.MaxPool2d() does the max pooling step.\n",
    "        ## kernel_size - The size of the filter (aka kernel) that does the\n",
    "        ##    max pooling. We're using a 2x2 grid for our filter.\n",
    "        ## stride - How much to move the filter each step. In this case\n",
    "        ##    we're moving it 2 units. Thus, our 2x2 filter does max pooling\n",
    "        ##    before moving 2 units over (or down). This means that our \n",
    "        ##    max pooling filter never overlaps itself.\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        ## Lastly, we create the \"normal\" neural network that has\n",
    "        ## 4 inputs, in_features=4, going to a single activation function, out_features=1, \n",
    "        ## in a single hidden layer...\n",
    "        self.input_to_hidden = nn.Linear(in_features=4, out_features=1)\n",
    "        ## ..and the single hidden layer, in_features=1, goes to\n",
    "        ## two outputs, out_features=2\n",
    "        self.hidden_to_output = nn.Linear(in_features=1, out_features=2)\n",
    "        \n",
    "        ## We'll use Cross Entropy to calculate the loss between what the \n",
    "        ## neural network's predictions and actual, or known, species for\n",
    "        ## each row in the dataset.\n",
    "        ## To learn more about Cross Entropy, see: https://youtu.be/6ArSys5qHAU\n",
    "        ## NOTE: nn.CrossEntropyLoss applies a SoftMax function to the values\n",
    "        ## we give it, so we don't have to do that oursevles. However,\n",
    "        ## when we use this neural network (after it has been trained), we'll\n",
    "        ## have to remember to apply a SoftMax function to the output.\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ## First we apply a filter to the input image\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        ## Then we run the output from the filter through a ReLU...\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        ## Then we run the output from the ReLU through a Max Pooling layer...\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        ## Now, at this point we have a square matrix of values.\n",
    "        ## So, in order to use those values as inputs to\n",
    "        ## a neural network, we use torch.flatten() to \n",
    "        ## turn the matrix into a vector.\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch \n",
    "\n",
    "        ## Now we run the flattened values through a neural network\n",
    "        ## with a single hidden layer and a single ReLU activation\n",
    "        ## function in that layer.\n",
    "        x = self.input_to_hidden(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.hidden_to_output(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        ## In this example, configuring the optimizer\n",
    "        ## consists of passing it the weights and biases we want\n",
    "        ## to optimize, which are all in self.parameters(),\n",
    "        ## and setting the learning rate with lr=0.001.\n",
    "        return Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ## The first thing we do is split 'batch'\n",
    "        ## into the input and label values.\n",
    "        inputs, labels = batch \n",
    "        \n",
    "        ## Then we run the input through the neural network\n",
    "        outputs = self.forward(inputs)\n",
    "        \n",
    "        ## Then we calculate the loss.\n",
    "        loss = self.loss(outputs, labels)\n",
    "        \n",
    "        ## Lastly, we could add the loss to a log file\n",
    "        ## so that we can graph it later. This would\n",
    "        ## help us decide if we have done enough training\n",
    "        ## Ideally, if we do enough training, the loss\n",
    "        ## should be small and not getting any smaller.\n",
    "        # self.log(\"loss\", loss) \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1ec20-c2d3-45f7-8ce7-e1c1df469f6d",
   "metadata": {},
   "source": [
    "Now that we've created a class for our neural network, let's train it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a6dd7-44ec-44ba-bffa-e0ab6eaa59d9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645699fb-e215-46f2-b730-2e9276d0a407",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"train\"></a>\n",
    "# Training our Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a3cac-ccb9-49d2-96fc-b797d7e0823c",
   "metadata": {},
   "source": [
    "Training our new convolutional neural network means we create a model from the new class, `SimpleCNN`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cb78b2a-cd0b-4072-97cb-664259e5aeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb353a83-3690-4a36-a164-e60e499bc79f",
   "metadata": {},
   "source": [
    "...and then create a **Lightning Trainer**, `L.Trainer()`, and use it to optimize the parameters. **NOTE:** We will start with 100 epochs, 100 complete runs through our training data. This may be enough to successfully optimize all of the parameters, but it might not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b994c8-46d0-4f06-a561-c49a75f5fda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/joshstarmer/Library/Python/3.9/lib/python/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Missing logger folder: /Users/joshstarmer/My Drive/stat_quests/book_statquest_illustrated_guide_to_neural_networks_and_ai/jupyter_notebooks/chapter_06/lightning_logs\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | conv             | Conv2d           | 10     | train\n",
      "1 | pool             | MaxPool2d        | 0      | train\n",
      "2 | input_to_hidden  | Linear           | 5      | train\n",
      "3 | hidden_to_output | Linear           | 4      | train\n",
      "4 | loss             | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------------------\n",
      "19        Trainable params\n",
      "0         Non-trainable params\n",
      "19        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/Users/joshstarmer/Library/Python/3.9/lib/python/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/joshstarmer/Library/Python/3.9/lib/python/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a0c14f9e794f80b933c5988f3c9182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=100)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0b74a-127c-49aa-8168-91dd28483b5f",
   "metadata": {},
   "source": [
    "Hooray! We've trained the model with 100 epochs! Now, let's see if the predictions are any good. We can do this by seeing how well it classifies the original training data. And we can do this using a `for` loop, just like we did earlier when we used to print out the contents of the **DataLoader** that we used for training.\n",
    "\n",
    "**NOTE:** The **[CrossEntropy](https://youtu.be/6ArSys5qHAU)** loss function we use during training applies a SoftMax function to the outputs. Thus, now that we're using the model, we'll apply `torch.softmax()` to its output. This will make it easier to read and interpret. And to make it even easier to read, we can run it through `torch.round()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f92d56c6-498d-47f6-adc4-ada1d52dfa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_label: tensor([[0.5200, 0.4800]], grad_fn=<RoundBackward1>)\n",
      "original label: tensor([[1., 0.]])\n",
      "\n",
      "\n",
      "predicted_label: tensor([[0.4200, 0.5800]], grad_fn=<RoundBackward1>)\n",
      "original label: tensor([[0., 1.]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_num, (image, label) in enumerate(dataloader):\n",
    "    \n",
    "    ## First, run the image through the model to make a prediction\n",
    "    prediction = model(image)\n",
    "    \n",
    "    ## Now make the prediction easy to read and interpret by\n",
    "    ## running it through torch.softmax() and torch.round()\n",
    "    predicted_label = torch.round(torch.softmax(prediction, dim=1), ## dim=0 applies softmax to rows, dim=1 applies soft to columns\n",
    "                                  decimals=2) \n",
    "    \n",
    "    ## Now print out the the predicted label and the original label\n",
    "    ## so we see how well our CNN performed.\n",
    "    print(\"predicted_label:\", predicted_label)\n",
    "    print(\"original label:\", label)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742bc94d-a7b7-4101-a8a7-38e6b2a525e2",
   "metadata": {},
   "source": [
    "As we can see in the output printed above, after 100 epochs of training, our convolutional neural network is not making good predictions. So let's do a lot more training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261fdd41-2caa-43be-8c78-0c2d64f08f3d",
   "metadata": {},
   "source": [
    "The good news is that because we're using **Lightning**, we can pick up where we left off training without starting over from scratch. This is because training with **Lightning** creates _checkpoint_ files that keep track of the Weights and Biases as they change. As a result, all we have to do to pick up where we left off is tell the `Trainer` where the checkpoint files are. This is awesome and will save us a lot of time since we don't have to retrain the first **100** epochs. So, let's add an additional **600** epochs to the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b8180-976d-4c1e-b2e7-cb45cba434c9",
   "metadata": {},
   "source": [
    "To add additional epochs to the training, we first identify where the checkpoint file is with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29bc4c1b-911d-417f-8ab2-9f52712502e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_checkpoint = trainer.checkpoint_callback.best_model_path ## By default, \"best\" = \"most recent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4fd497-997c-4369-9b3e-d25cd0911d5d",
   "metadata": {},
   "source": [
    "Then we create a new **Lightning Trainer**, just like before, but we set the number of epochs to 700. Given that we already trained for 100 epochs, this means we'll do 600 more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a849d2f-bad1-4214-a72b-f1b04174639c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at /Users/joshstarmer/My Drive/stat_quests/book_statquest_illustrated_guide_to_neural_networks_and_ai/jupyter_notebooks/chapter_06/lightning_logs/version_0/checkpoints/epoch=99-step=200.ckpt\n",
      "/Users/joshstarmer/Library/Python/3.9/lib/python/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:360: The dirpath has changed from '/Users/joshstarmer/My Drive/stat_quests/book_statquest_illustrated_guide_to_neural_networks_and_ai/jupyter_notebooks/chapter_06/lightning_logs/version_0/checkpoints' to '/Users/joshstarmer/My Drive/stat_quests/book_statquest_illustrated_guide_to_neural_networks_and_ai/jupyter_notebooks/chapter_06/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | conv             | Conv2d           | 10     | train\n",
      "1 | pool             | MaxPool2d        | 0      | train\n",
      "2 | input_to_hidden  | Linear           | 5      | train\n",
      "3 | hidden_to_output | Linear           | 4      | train\n",
      "4 | loss             | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------------------\n",
      "19        Trainable params\n",
      "0         Non-trainable params\n",
      "19        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at /Users/joshstarmer/My Drive/stat_quests/book_statquest_illustrated_guide_to_neural_networks_and_ai/jupyter_notebooks/chapter_06/lightning_logs/version_0/checkpoints/epoch=99-step=200.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69f21687e51490b828c9d83d602272c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=700` reached.\n"
     ]
    }
   ],
   "source": [
    "## First, create a new Lightning Trainer\n",
    "trainer = L.Trainer(max_epochs=700) # Before, max_epochs=100, so, by setting it to 700, we're adding 600 more.\n",
    "\n",
    "## Then call trainer.fit() using the path to the most recent checkpoint files\n",
    "## so that we can pick up where we left off.\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca3ecd4-f56d-4a1e-89cc-1f7cf5992a1e",
   "metadata": {},
   "source": [
    "Now, let's see if the predictions have improved after 700 epochs. We'll do this using the same `for` loop that we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cabdaf11-e107-4b3e-bda8-d6b8d3d820d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_label: tensor([[0.7900, 0.2100]], grad_fn=<RoundBackward1>)\n",
      "original label: tensor([[1., 0.]])\n",
      "\n",
      "\n",
      "predicted_label: tensor([[0.0100, 0.9900]], grad_fn=<RoundBackward1>)\n",
      "original label: tensor([[0., 1.]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_num, (image, label) in enumerate(dataloader):\n",
    "    \n",
    "    ## First, run the image through the model to make a prediction\n",
    "    prediction = model(image)\n",
    "    \n",
    "    ## Now make the prediction easy to read and interpret by\n",
    "    ## running it through torch.softmax() and torch.round()\n",
    "    predicted_label = torch.round(torch.softmax(prediction, dim=1), ## dim=0 applies softmax to rows, dim=1 applies soft to columns\n",
    "                                  decimals=2) \n",
    "    \n",
    "    ## Now print out the the predicted label and the original label\n",
    "    ## so we see how well our CNN performed.\n",
    "    print(\"predicted_label:\", predicted_label)\n",
    "    print(\"original label:\", label)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31918cf1-5cfc-475b-b61a-f08525111741",
   "metadata": {},
   "source": [
    "After 700 training epochs, the predicted values are much, much closer to the ideal values. And if we rounded them to the nearest whole number, we'd get the exact labels that we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a78589d-3d39-40a4-81d1-c22caddc6879",
   "metadata": {},
   "source": [
    "# Double BAM!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8da8b1-4b5e-42a8-bdaa-3d939716fad2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b568a1ad-f6d4-436f-8c58-b39371680175",
   "metadata": {},
   "source": [
    "<a id=\"predict\"></a>\n",
    "# Make a Prediction with New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2df2a0-77e3-40dd-8cf5-3a23d66e5098",
   "metadata": {},
   "source": [
    "Now that our model is trained, we can use it to make predictions from new data. Specifically, let's see how well our model predicts an image of the letter **X** that has been shifted over one pixel. First, let's create the image of an **X** shifted by one pixel to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f2a2fe8-3418-4adc-93ea-a95ecb717b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 1],\n",
       " [0, 0, 0, 1, 1, 0],\n",
       " [0, 0, 0, 1, 1, 0],\n",
       " [0, 0, 1, 0, 0, 1],\n",
       " [0, 1, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifted_x_image = [[0, 1, 0, 0, 0, 0],\n",
    "                   [0, 0, 1, 0, 0, 1],\n",
    "                   [0, 0, 0, 1, 1, 0],\n",
    "                   [0, 0, 0, 1, 1, 0],\n",
    "                   [0, 0, 1, 0, 0, 1],\n",
    "                   [0, 1, 0, 0, 0, 0]]\n",
    "shifted_x_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe61a6-a644-419c-a23f-17b1415dd973",
   "metadata": {},
   "source": [
    "Now, let's verify that we created the correct image by drawing with matplotlib, just like we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48ad7c2a-ff39-4cfb-bb69-2722b723db6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14571e430>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## When we only want to draw one image, we can omit 'nrows' and 'ncols' and \n",
    "## a single drawing element, ax, is returned.\n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.imshow(shifted_x_image, cmap='gray_r') ## Setting cmap='gray_r' gives us reverse grayscale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed44d62a-d54c-41c6-b73a-1301b4a6d003",
   "metadata": {},
   "source": [
    "Now that we can see that we correctly created an image of an **X** shifted one pixel to the right, let's see whether or not our trained convolutional neural network can correctly classify it as an **X**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "616a977c-ffc3-43b0-8f33-096a8284c162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1200, 0.8800]], grad_fn=<RoundBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## First, let's make a prediction with the new image...\n",
    "prediction = model(torch.tensor([shifted_x_image]).type(torch.float32))\n",
    "\n",
    "## Now make the prediction easy to read and interpret by\n",
    "## running it through torch.softmax() and torch.round()\n",
    "predicted_label = torch.round(torch.softmax(prediction, dim=1), decimals=2) ## dim=0 applies argmax to rows, dim=1 applies argmax to colum\n",
    "\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1572bb9-c25e-47a8-baf3-a7c09e18f0c8",
   "metadata": {},
   "source": [
    "And we see that the trained network correctly predicted **X** since the second output value, which represents **X**, is larger than the first output value, which represents **O**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf5033-422d-4731-bedb-d49b4e82e47b",
   "metadata": {},
   "source": [
    "Now, let's see if our trained network can correctly classify an image of the letter **O** that is shifted one pixel to the left. We'll do this just like we did for the shifted **X**. So, the first thing we do is create the image..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32266790-ceda-4ab0-a5e9-2888961c2053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 1, 0, 0, 0],\n",
       " [1, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 1, 0, 0],\n",
       " [0, 1, 1, 0, 0, 0]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifted_o_image = [[0, 1, 1, 0, 0, 0],\n",
    "                   [1, 0, 0, 1, 0, 0],\n",
    "                   [0, 0, 0, 0, 1, 0],\n",
    "                   [0, 0, 0, 0, 1, 0],\n",
    "                   [1, 0, 0, 1, 0, 0],\n",
    "                   [0, 1, 1, 0, 0, 0]]\n",
    "shifted_o_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33752345-a5ff-4154-b402-632ff9d4cfe7",
   "metadata": {},
   "source": [
    "...then we display the image to verify that we made it correctly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f3ba40a-7912-44e8-96f8-9bfea66c56dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1694ca4f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## When we only want to draw one image, we can omit 'nrows' and 'ncols' and \n",
    "## a single drawing element, ax, is returned.\n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.imshow(shifted_o_image, cmap='gray_r') ## Setting cmap='gray_r' gives us reverse grayscale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e1c572-113a-4685-bef7-2926783b21c9",
   "metadata": {},
   "source": [
    "Lastly, we run the image through the trained network to make a prediction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e849cbb-d3e2-4e2a-a7cd-960db3d0630a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4500, 0.5500]], grad_fn=<RoundBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make a prediction with the new image...\n",
    "prediction = model(torch.tensor([shifted_o_image]).type(torch.float32))\n",
    "\n",
    "## Now make the prediction easy to read and interpret by\n",
    "## running it through torch.softmax() and torch.round()\n",
    "predicted_label = torch.round(torch.softmax(prediction, dim=1), decimals=2) ## dim=0 applies argmax to rows, dim=1 applies argmax to colum\n",
    "\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f076ae9-7efb-4ffb-ae6f-3793d28fb655",
   "metadata": {},
   "source": [
    "And it looks like our network thought that the shifted **O** was an **X**. Oh well.\n",
    "\n",
    "Small bam.\n",
    "\n",
    "However, we learned a lot, and with more training data and more variation in the letters, we might be able to make better predictions in the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2940946-b17a-4e9b-ac7f-02cd4f97db91",
   "metadata": {},
   "source": [
    "# TRIPLE BAM!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
